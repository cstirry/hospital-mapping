[
  {
    "objectID": "notebooks/analysis.html",
    "href": "notebooks/analysis.html",
    "title": "Get Data",
    "section": "",
    "text": "import requests\nimport pandas as pd\nimport geopandas as gpd\n\n\n# Data Sources\n\n# CMS - General Hospital Info\n# https://data.cms.gov/provider-data/dataset/xubh-q36u\nhosp_csv_url = 'https://data.cms.gov/provider-data/sites/default/files/resources/893c372430d9d71a1c52737d01239d47_1729022728/Hospital_General_Information.csv'\n\n# CMS - Maternal Health\n# https://data.cms.gov/provider-data/dataset/nrdb-3fcy\nmat_csv_url = \"https://data.cms.gov/provider-data/sites/default/files/resources/5a4754b088fdb10d2ae278ef215925a7_1729022763/Maternal_Health-Hospital.csv\"\n\n# ArcGIS Hospital Data API\n# https://hifld-geoplatform.hub.arcgis.com/\nhifld_url = \"https://services1.arcgis.com/Hp6G80Pky0om7QvQ/arcgis/rest/services/Hospitals_gdb/FeatureServer/0/query\"\n\n# Shapefile Paths\n# downloaded from https://www.weather.gov/gis/AWIPSShapefiles\nstate_shapefile_path = 'shapefiles/s_18mr25/s_18mr25.shp'\ncounty_shapefile_path = 'shapefiles/c_18mr25/c_18mr25.shp'\n\n\ndef download_csv_to_pandas(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad status codes\n        \n        # Load the CSV data if the request was successful\n        df = pd.read_csv(url)\n        print(f\"Data from {url} loaded successfully\", df.shape)\n        return df\n    except requests.exceptions.HTTPError as http_err:\n        print(f\"HTTP error occurred while fetching {url}: {http_err}\")\n    except Exception as err:\n        print(f\"An error occurred while fetching {url}: {err}\")\n    return None\n\n\ndef download_arcgis_api_to_pandas(url):\n    params = {\n        \"where\": \"1=1\",       \n        \"outFields\": \"*\",    \n        \"f\": \"geojson\"        \n    }\n    \n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()  # Raise an error for bad status codes\n\n        # Load GeoJSON data into a GeoDataFrame\n        gdf = gpd.read_file(response.text)\n        print(f\"Data from {url} loaded successfully\", gdf.shape)\n        return gdf\n    except requests.exceptions.HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error occurred: {err}\")\n    return None\n\ncms_mat_df = download_csv_to_pandas(mat_csv_url)\ncms_hosp_df = download_csv_to_pandas(hosp_csv_url)\nhifld_df = download_arcgis_api_to_pandas(hifld_url)\n\n\nCreate US & Maryland Maps\n\nimport folium\nfrom folium.plugins import HeatMap\n\n\n# US Heatmap\nm = folium.Map(location=[39.8283, -98.5795], zoom_start=4, tiles='Cartodb Positron')\n\ndf = hifld_df\ndf['lon'] = df['geometry'].apply(lambda point: point.x)\ndf['lat'] = df['geometry'].apply(lambda point: point.y)\nheat_data = df[['lat', 'lon']].values.tolist()\nHeatMap(heat_data, radius=2, blur=1).add_to(m)\n\nm.save('outputs/us_heatmap.html')\n\n\n# Maryland Heatmap\nm = folium.Map(location=[39.0458, -76.6413], zoom_start=8, tiles='Cartodb Positron')\n\n# Hospital data\ndf = hifld_df\nmd_df = df[df['STATE'] == 'MD'].copy()\nmd_df['lon'] = md_df['geometry'].apply(lambda point: point.x)\nmd_df['lat'] = md_df['geometry'].apply(lambda point: point.y)\n\nheat_data = md_df[['lat', 'lon']].values.tolist()\nHeatMap(heat_data, radius=8, blur=4).add_to(m)\n\n# Add Maryland boundary to the map\nstates_gdf = gpd.read_file(state_shapefile_path)\nmd_boundary = states_gdf[states_gdf['NAME'] == 'Maryland']\n\nfolium.GeoJson(\n    md_boundary,\n    style_function=lambda feature: {\n        'fillColor': 'none',  \n        'color': 'blue',   \n        'weight': 1        \n    }\n).add_to(m)\n\nm.save('outputs/maryland_heatmap.html')\n\n\n# Maryland Cloropath\nm = folium.Map(location=[39.0458, -76.6413], zoom_start=8, tiles='Cartodb Positron')\n\n# Load the shapefile for US counties\ncounties_gdf = gpd.read_file(county_shapefile_path)\nmd_counties = counties_gdf[counties_gdf['STATE'] == 'MD']\n\n# Filter hospital data for Maryland\nmd_df = df[df['STATE'] == 'MD'].copy()\n\n# Spatial join to assign counties to hospitals\nmd_counties = md_counties.to_crs(epsg=4326)\nmd_df = gpd.sjoin(md_df, md_counties, how=\"left\", predicate='within')\n\n# Aggregate count of hospitals by County\nhospital_count_by_county = md_df.groupby('COUNTYNAME').size().reset_index(name='HOSPITAL_COUNT')\n\n# Merge aggregated hospital count data with county shapefile\nmd_counties = md_counties.merge(hospital_count_by_county, on='COUNTYNAME')\n\n# Create Choropleth map based on hospital count by county\nfolium.Choropleth(\n    geo_data=md_counties,\n    data=md_counties,\n    columns=['COUNTYNAME', 'HOSPITAL_COUNT'],\n    key_on='feature.properties.COUNTYNAME',\n    fill_color='YlGn',\n    fill_opacity=0.7,\n    line_opacity=0.3,\n    legend_name='Number of Hospitals by County',\n    highlight=True,\n    bins=[1, 3, 5, 10, 15, 20],\n).add_to(m)\n\n# Save or display the map\nm.save('outputs/maryland_choropleth.html')\n\n\n# Maryland Dot Map\nm = folium.Map(location=[39.0458, -76.6413], zoom_start=8, tiles='Cartodb Positron')\n\n# Hospital data\ndf = hifld_df\nmd_df = df[df['STATE'] == 'MD'].copy()\nmd_df['lon'] = md_df['geometry'].apply(lambda point: point.x)\nmd_df['lat'] = md_df['geometry'].apply(lambda point: point.y)\n\nfor idx, row in md_df.iterrows():\n    folium.CircleMarker(\n        location=[row['lat'], row['lon']],\n        radius=4,\n        color='red',\n        fill=True,\n        fill_opacity=0.7,\n        popup=row.get('NAME', 'Hospital')\n    ).add_to(m)\n\n# Add Maryland boundary to the map\nstates_gdf = gpd.read_file(state_shapefile_path)\nmd_boundary = states_gdf[states_gdf['NAME'] == 'Maryland']\n\nfolium.GeoJson(\n    md_boundary,\n    style_function=lambda feature: {\n        'fillColor': 'none',  \n        'color': 'blue',   \n        'weight': 1        \n    }\n).add_to(m)\n\nm.save('outputs/maryland_marker.html')\n\n\n\nCombine Datasets\n\nfrom fuzzywuzzy import process, fuzz\nimport numpy as np\n\n\n# Clean Data\nhifld_df_md = hifld_df[hifld_df['STATE'] == 'MD']\ncms_hosp_df_md = cms_hosp_df[cms_hosp_df['State'] == 'MD']\ncms_mat_df_md = cms_mat_df[cms_mat_df['State'] == 'MD']\n\ncms_hosp_df_md = cms_hosp_df_md[['Facility ID', 'Facility Name', 'Address', 'City/Town', 'State', 'ZIP Code', 'County/Parish',\n'Hospital Type','Hospital Ownership', 'Emergency Services','Meets criteria for birthing friendly designation','Hospital overall rating']]\n\ncms_mat_df_md['Score'] = pd.to_numeric(cms_mat_df_md['Score'], errors='coerce')\ncms_mat_df_md = cms_mat_df_md.pivot_table(index=['Facility ID', 'Facility Name', 'Address', 'City/Town', 'State', 'ZIP Code'],\n                         columns='Measure Name',\n                         values='Score').reset_index()\ncms_mat_df_md.columns.name = None\ncms_mat_df_md.columns = [str(col) for col in cms_mat_df_md.columns]\n\n\ndf1 = hifld_df_md \ndf2 = cms_hosp_df_md\ndf3 = cms_mat_df_md\n\ndf1.rename(columns={\n    'NAME': 'Facility Name',\n    'ADDRESS': 'Address',\n    'CITY': 'City/Town',\n    'STATE': 'State',\n    'ZIP': 'ZIP Code'\n}, inplace=True)\n\ndf1['ZIP Code'] = df1['ZIP Code'].astype(str)\ndf2['ZIP Code'] = df2['ZIP Code'].astype(str)\ndf3['ZIP Code'] = df3['ZIP Code'].astype(str)\n\n# Combine CMS datasets\ndf2_df3_merged = pd.merge(df2, df3, on=['Address', 'City/Town', 'ZIP Code'], how='outer', suffixes=('_df2', '_df3'))\nfor col in df2.columns:\n    if col in df3.columns and col not in ['Address', 'City/Town', 'ZIP Code']:\n        col_df2 = f\"{col}_df2\"\n        col_df3 = f\"{col}_df3\"\n        \n        if col_df2 in df2_df3_merged.columns and col_df3 in df2_df3_merged.columns:\n            # Where values are equal or one is NaN, fill NaN or keep one column\n            df2_df3_merged[col] = df2_df3_merged[col_df2].combine_first(df2_df3_merged[col_df3])\n            \n            # Drop the old columns with suffixes\n            df2_df3_merged.drop(columns=[col_df2, col_df3], inplace=True)\n\n# Fuzzy Matching Addresses Between df1 and df2_df3_merged\ndef get_best_match(address, choices, threshold=85):\n    \"\"\"Find the best fuzzy match for an address.\"\"\"\n    match, score = process.extractOne(address, choices, scorer=fuzz.token_sort_ratio)\n    return match if score &gt;= threshold else None\n\naddress_choices = df2_df3_merged['Address'].dropna().unique()\ndf1['Fuzzy_Matched_Address'] = df1['Address'].apply(lambda x: get_best_match(x, address_choices))\n\n# Combine all\nfinal_merge = pd.merge(df1, df2_df3_merged, left_on=['Fuzzy_Matched_Address', 'ZIP Code'],\n                       right_on=['Address', 'ZIP Code'], how='outer', suffixes=('_cms', '_hifld'))\n\nprint(df1.count().max())\nprint(df2.count().max())\nprint(df3.count().max())\nprint(final_merge.count().max())\n\n\n# Helper function to consolidate primary values, ignoring 'Not Available'\ndef consolidate(primary, alt1, alt2=None):\n    values = [primary, alt1, alt2]\n    # Filter out NaN and 'Not Available'\n    values = [v for v in values if pd.notna(v) and v != 'Not Available']\n    return values[0] if values else np.nan  # Return first valid value or NaN if none\n\n# Helper function to get alternatives, ignoring 'Not Available'\ndef get_alternative(primary, alt1, alt2=None):\n    alternatives = []\n    for alt in [alt1, alt2]:\n        if pd.notna(alt) and alt != 'Not Available' and alt != primary:\n            alternatives.append(alt)\n        else:\n            alternatives.append(np.nan)\n    return alternatives[0]  # Return the first valid alternative, or NaN if none\n\n# Consolidate Facility Name\nfinal_merge['Facility Name'] = final_merge.apply(\n    lambda row: consolidate(row['Facility Name_cms'], row['Facility Name_hifld'], row['ALT_NAME']), axis=1)\n\nfinal_merge['Facility Name_alt'] = final_merge.apply(\n    lambda row: get_alternative(row['Facility Name'], row['Facility Name_hifld'], row['ALT_NAME']), axis=1)\n\n# Consolidate Address\nfinal_merge['Address'] = final_merge.apply(\n    lambda row: consolidate(row['Address_cms'], row['Address_hifld'], row['Fuzzy_Matched_Address']), axis=1)\n\nfinal_merge['Address_alt'] = final_merge.apply(\n    lambda row: get_alternative(row['Address'], row['Address_hifld'], row['Fuzzy_Matched_Address']), axis=1)\n\n# Consolidate City/Town\nfinal_merge['City/Town'] = final_merge.apply(\n    lambda row: consolidate(row['City/Town_cms'], row['City/Town_hifld']), axis=1)\n\nfinal_merge['City/Town_alt'] = final_merge.apply(\n    lambda row: get_alternative(row['City/Town'], row['City/Town_hifld']), axis=1)\n\n# Consolidate State\nfinal_merge['State'] = final_merge.apply(\n    lambda row: consolidate(row['State_cms'], row['State_hifld']), axis=1)\n\nfinal_merge['State_alt'] = final_merge.apply(\n    lambda row: get_alternative(row['State'], row['State_hifld']), axis=1)\n\n# Consolidate County\nfinal_merge['County'] = final_merge.apply(\n    lambda row: consolidate(row['County/Parish'], row['COUNTY']), axis=1)\n\nfinal_merge['County_alt'] = final_merge.apply(\n    lambda row: get_alternative(row['County'], row['COUNTY']), axis=1)\n\n# Keep ZIP Code and ZIP4 as is\nfinal_merge['ZIP Code'] = final_merge['ZIP Code'].fillna(final_merge['ZIP4'])\n\nfinal_merge.drop(columns=[col for col in final_merge.columns if col.endswith('_cms') or col.endswith('_hifld')], inplace=True)\n\n\nfinal_merge = final_merge[['OBJECTID', 'ID', 'Facility ID', 'Facility Name', 'Facility Name_alt',\n       'Address', 'Address_alt', 'City/Town', 'City/Town_alt', 'State',\n       'State_alt', 'County', 'County_alt', 'ZIP Code', 'ZIP4', 'TYPE',\n       'OWNER', 'Hospital Type', 'Hospital Ownership', 'STATUS', 'POPULATION',\n       'NAICS_CODE', 'NAICS_DESC', 'WEBSITE', 'TELEPHONE', 'TTL_STAFF', 'BEDS',\n       'TRAUMA', 'HELIPAD', 'Emergency Services',\n       'Meets criteria for birthing friendly designation',\n       'Hospital overall rating', 'Cesarean Birth', 'Elective Delivery',\n       'Exclusive Breast Milk Feeding',\n       'Risk Adjusted Severe Obstetric Complications (All)',\n       'Risk Adjusted Severe Obstetric Complications (excluding blood-transfusion-only cases)',\n       'LATITUDE', 'LONGITUDE', 'geometry']]\n\n\n\nClean up Coordinates\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n\n# Clean Up Coordinates\ndf = final_merge\ndef extract_coords(row):\n    if pd.isna(row['LATITUDE']) or pd.isna(row['LONGITUDE']):\n        if pd.notna(row['geometry']):\n            # Extract longitude and latitude from 'geometry' string (format: 'POINT (lon lat)')\n            lon, lat = row['geometry'].replace('POINT (', '').replace(')', '').split()\n            return pd.Series({'LATITUDE': float(lat), 'LONGITUDE': float(lon)})\n    return pd.Series({'LATITUDE': row['LATITUDE'], 'LONGITUDE': row['LONGITUDE']})\n\n# Apply extraction to fill missing LAT/LON\ndf[['LATITUDE', 'LONGITUDE']] = df.apply(extract_coords, axis=1)\n\n\n# Initialize geocoder with a user agent\ngeolocator = Nominatim(user_agent=\"hospital_locator\")\n\n# Function to build the full address string from available fields\ndef build_full_address(row):\n    parts = [\n        row.get('Address') or row.get('Address_alt'), \n        row.get('City/Town') or row.get('City/Town_alt'),\n        row.get('County') or row.get('County_alt'),\n        row.get('State') or row.get('State_alt'),\n        row.get('ZIP Code') or row.get('ZIP4')\n    ]\n    # Filter out None or NaN values and join into a single string\n    return ', '.join([str(part) for part in parts if pd.notna(part)])\n\n# Function to geocode missing LATITUDE and LONGITUDE\ndef geocode_missing(row):\n    if pd.isna(row['LATITUDE']) or pd.isna(row['LONGITUDE']):\n        full_address = build_full_address(row)\n        try:\n            location = geolocator.geocode(full_address, timeout=10)\n            if location:\n                return pd.Series({'LATITUDE': location.latitude, 'LONGITUDE': location.longitude})\n        except Exception as e:\n            print(f\"Error geocoding address: {full_address}, Error: {e}\")\n        # Return NaN if geocoding fails\n        return pd.Series({'LATITUDE': np.nan, 'LONGITUDE': np.nan})\n    return pd.Series({'LATITUDE': row['LATITUDE'], 'LONGITUDE': row['LONGITUDE']})\n\n# Apply geocoding to rows with missing coordinates\ndf[['LATITUDE', 'LONGITUDE']] = df.apply(geocode_missing, axis=1)\n\n# Ensure rate limits are respected by pausing between requests\ntime.sleep(1) \n\n\n\nBaltimore\n\nfrom geopy.distance import geodesic\nfrom folium import FeatureGroup, LayerControl\n\n\n# Coordinates for the center of Baltimore City\nbaltimore_coords = (39.2904, -76.6122)\n\n# Function to calculate distance from Baltimore\ndef within_15_miles(row):\n    if pd.notnull(row['LATITUDE']) and pd.notnull(row['LONGITUDE']):\n        hospital_coords = (row['LATITUDE'], row['LONGITUDE'])\n        distance = geodesic(baltimore_coords, hospital_coords).miles\n        return distance &lt;= 15\n    return False\n\nbmore_df = df[df.apply(within_15_miles, axis=1)]\nbmore_df = bmore_df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n\n\n# Initialize the Map\nm = folium.Map(location=[39.2904, -76.6122], zoom_start=9.5)\n\n# Layer 1: Birthing Friendly Designation\nbirthing_layer = FeatureGroup(name='Birthing Friendly Designation')\nfor _, row in bmore_df.iterrows():\n    color = 'green' if row['Meets criteria for birthing friendly designation'] == 'Y' else 'red'\n    \n    popup_content = f\"\"\"\n    {row['Facility Name']}&lt;br&gt;\n    Birthing Friendly: {row['Meets criteria for birthing friendly designation']}&lt;br&gt;\n    Hospital Rating: {row['Hospital overall rating']}\n    \"\"\"\n\n    folium.Marker(\n        location=[row['LATITUDE'], row['LONGITUDE']],\n        popup=popup_content,\n        icon=folium.Icon(color=color)\n    ).add_to(birthing_layer)\n\nbirthing_layer.add_to(m)\n\n\n# Add Custom Legend\nlegend_html = '''\n&lt;div style=\"position: fixed; \n     bottom: 50px; left: 50px; width: 200px; height: 90px; \n     background-color: white; z-index:9999; font-size:12px;\n     border:2px solid grey; border-radius:8px; padding: 10px;\"&gt;\n     &lt;b&gt;Birthing Friendly Hospital&lt;/b&gt;&lt;br&gt;\n     &lt;i class=\"fa fa-map-marker fa-2x\" style=\"color:green\"&gt;&lt;/i&gt; Yes&lt;br&gt;\n     &lt;i class=\"fa fa-map-marker fa-2x\" style=\"color:red\"&gt;&lt;/i&gt; No\n&lt;/div&gt;\n'''\n\nm.get_root().html.add_child(folium.Element(legend_html))\n\nradius_meters = 15 * 1609.34\n\nfolium.Circle(\n    location=baltimore_coords,\n    radius=radius_meters,\n    color='blue',\n    fill=True,\n    fill_opacity=0.1,\n    popup='15-Mile Radius from Baltimore'\n).add_to(m)\n\n# Add a marker for Baltimore center\nfolium.Marker(\n    location=baltimore_coords,\n    popup='Baltimore City Center',\n    icon=folium.Icon(color='blue')\n).add_to(m)\n\nm.save('outputs/baltimore.html')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hospital Data",
    "section": "",
    "text": "This project explores and maps hospital-level data from various sources (CMS & HIFLD), presented in a Quarto document. It demonstrates the application of GIS techniques using Python libraries to analyze and visualize data. Key methods include acquiring data from APIs, geocoding addresses into coordinates, calculating geospatial distances, working with shapefiles to define geographic boundaries, merging datasets with fuzzy matching to resolve address inconsistencies, and creating interactive maps with Folium.\n\n\n\n\n\n\n\n\n\nShow Heatmap Show Choropleth Map Show Dot Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHospitals within 15 miles of downtown Baltimore"
  },
  {
    "objectID": "index.html#maryland-maps",
    "href": "index.html#maryland-maps",
    "title": "Hospital Data",
    "section": "",
    "text": "Show Heatmap Show Choropleth Map Show Dot Map"
  },
  {
    "objectID": "index.html#baltimore-map",
    "href": "index.html#baltimore-map",
    "title": "Hospital Data",
    "section": "",
    "text": "Hospitals within 15 miles of downtown Baltimore"
  }
]